---
title: "ADM_Assignment_3"
author: "vdodda@kent.edu"
date: "`r Sys.Date()`"
output:
  word_document: default
  pdf_document: default
---
```{r}

```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
**============================PART-A==============================**
  
**Question 1: What is the difference between SVM with hard margin and soft margin?**

SVM (Support Vector Machine) is a machine learning algorithm used for classification and regression analysis. SVM with hard margin and soft margin are two variants of the SVM algorithm, differing in the degree of flexibility allowed in the placement of the decision boundary.

Hard margin SVM aims to find a decision boundary that separates the two classes with the maximum margin, without allowing any misclassifications or errors. This can only be done when the data is linearly separable, meaning there is a clear gap between the two classes. The maximum margin is defined as the minimum distance from the decision boundary to the closest training data points. Hard margin SVM can be sensitive to outliers and prone to overfitting, especially when the margin is small.

Soft margin SVM allows for some degree of misclassification or errors in the training data, by introducing a penalty term for misclassifications. This means that the algorithm is more flexible in finding a decision boundary that can handle non-linearly separable data. The degree of flexibility is controlled by the regularization parameter C, which determines the trade-off between maximizing the margin and minimizing the misclassification errors. A larger C value leads to a smaller margin and fewer misclassifications, while a smaller C value leads to a larger margin and more misclassifications. Soft margin SVM is generally preferred over hard margin SVM in real-world applications, where the data is often noisy or overlapping, and linearly separable data is rare.

The difference between hard margin and soft margin SVMs lies in the separability of the data. If the data is linearly separable, hard margin SVM can be used to find the decision boundary with maximum margin without allowing any misclassifications. However, if the data is not linearly separable, soft margin SVM is more appropriate because it allows some degree of misclassification while finding the decision boundary that can separate the classes as well as possible. Soft margin SVM can also be used when the data is linearly separable, but the margin is too small, and the model is at risk of overfitting or being too sensitive to outliers.

In summary, hard margin SVM and soft margin SVM are two variants of SVM that differ in their approach to finding the decision boundary. Hard margin SVM aims to find the decision boundary with maximum margin without allowing any misclassifications, while soft margin SVM allows some degree of misclassification to handle non-linearly separable data. Soft margin SVM is generally preferred in real-world applications due to its flexibility and ability to handle noisy or overlapping data.


**Question 2: What is the role of the cost parameter, C, in SVM (with soft margin) classifiers? **

In SVM with soft margin classifiers, the cost parameter, C, plays a crucial role in controlling the trade-off between maximizing the margin and minimizing the classification error. It is a regularization parameter that assigns weight to the constraints and determines how strictly the model should follow them.

A lower C value corresponds to a softer margin, which allows more misclassifications. This means that the model is more tolerant of errors and places more emphasis on finding the largest margin. On the other hand, a higher C value leads to a stricter margin, which results in fewer misclassifications. The model tries to minimize misclassifications on the training set, and the margin may become smaller.

When C is set to infinity, there are no slack variables, and the model strictly follows the constraints. In this case, all training data points must be correctly classified, which is similar to the hard margin SVM.

In summary, the cost parameter, C, controls how much the model is penalized for misclassifying data points and determines the balance between margin size and classification error. The choice of C value depends on the specific problem and the trade-off between model complexity and accuracy that is desired. A smaller C value may result in a simpler model, while a larger C value may lead to a more complex model with higher accuracy on the training set.



**Question 3: Will the following perceptron be activated (2.8 is the activation threshold)**

The perceptron algorithm is a linear classifier that applies a weighted sum of inputs and a threshold function to make predictions. The perceptron's output is determined by the sign of the weighted sum, and the threshold is used to determine if the output should be a +1 or -1.

In this case, the perceptron's weighted sum is calculated as (0.10.8) + (11.1(-0.2)) = 0.08 - 2.22 = -2.14. Since the result is negative, the perceptron's activation function will output -1. Therefore, the perceptron will not be activated.


**Question4: What is the role of alpha, the learning rate in the delta rule?**

The delta rule is a learning algorithm that uses gradient descent to search for the best weights that fit the training examples. The learning rate, alpha, plays an important role in the delta rule as it determines how fast the weights should be updated. A higher learning rate results in faster changes to the weights, while a lower learning rate leads to smoother changes and a more stable operation. The optimal learning rate results in the fastest convergence of the algorithm. Therefore, it is usually recommended to start with a high learning rate to adapt quickly and then gradually decrease it to fine-tune the weights.

The quantity of gradient we consider when using gradient descent is determined by the learning rate, alpha. The higher the alpha, the larger portion of the current gradient is considered, and the smaller the alpha, the smaller the considered gradient. This is why alpha is called the learning rate because it determines the speed of weight updates as a function of the inputs, outputs, and target. A higher alpha result in faster weight changes, while a lower alpha leads to smoother changes. It is generally a good idea to use a high learning rate initially to get closer to the target and then switch to a smaller rate to smoothly reach the optimal weight values.


**============================PART-B==============================**

## Importing the required Packages

```{r}
library(dplyr)
library(ISLR)
library(glmnet)
library(caret)
library(kernlab)
```


## Selecting the required attributes.
```{r}
Carseats_required = Carseats %>% select("Sales", "Price","Advertising","Population","Age","Income","Education")
```

**Question B1. Build a linear SVM regression model to predict Sales based on all other attributes ("Price", "Advertising", "Population", "Age", "Income" and "Education"). Hint: use caret train() with method set to  “svmLinear”. What is the R-squared of the model?**

```{r}
tc = trainControl(method = "repeatedcv", number = 10, repeats = 3)
set.seed(143)
linearsvm = train(Sales~., data = Carseats_required, method = "svmLinear",
 trControl=tc,preProcess = c("center", "scale"),tuneLength = 10)
linearsvm
```
## In the above model, I added a train control section that adds cross validation to the model.
## R-squared value is 0.3532603

**Question B2 - Customize the search grid by checking the model’s performance for C parameter of0.1,.5,1 and 10 using 2 repeats of 5-fold cross validation?** 

```{r}
grid = expand.grid(C = c(0.1,0.5,1,10))

tc2 = trainControl(method = "repeatedcv", number = 5, repeats = 2)

linearsvm_grid = train(Sales~., data = Carseats_required, method = "svmLinear",trControl=tc2,preProcess = c("center", "scale"),tuneGrid = grid,tuneLength = 10)
linearsvm_grid
```
## I have customized the search grid with the given values. The number of folds used is 5 with total repeats of 2. Hence, the best value for c when rmse considered is c=0.1  (rmse= 2.277129).

**Question B3 - Train a neural network model to predict Sales based on all other attributes ("Price", "Advertising","Population", "Age", "Income" and "Education"). Hint: use caret train() with method set to “nnet”. What is the R-square of the model with the best hyper parameters (using default caret search grid) – hint: don’t forget to scale the data?**

```{r}
set.seed(6354)
number_of_folds <- trainControl(method = 'LOOCV', verboseIter = FALSE)
carseats_nnet <- train(Sales~., data = Carseats_required, method = "nnet",
preProcess = c("center", "scale"),trControl = number_of_folds)
```

```{r}
carseats_nnet
```

## The final values(best hyper parameter)used for the model are size=1 and decay=1e-04.The R-squared for the selected value is "NA".

**Question B4 - Consider the following input: Sales=9, Price=6.54, Population=124, Advertising=0, Age=76, Income=110, Education=10 What will be the estimated Sales for this record using the above neuralnet model?**

```{r}
 Sales = c(9)
 Price = c(6.54)
 Population = c(124)
 Advertising = c(0)
 Age = c(76)
 Income = c(110)
 Education= c(10)
 
 testset1 = data.frame(Sales, Price, Population, Advertising, Age, Income, Education)

 Predict_sales = predict(carseats_nnet, testset1)
Predict_sales
```
## Based on the given instructions, the Neural Net predicts that there will only be one sale with the provided record. This outcome is concerning because in the previous task, the decision tree estimated that 9.5 sales would take place with the same record. It is my opinion that constructing a neural network model using the "keras_model_sequential" and Keras package may result in a more adaptable model and ultimately lead to a more favorable outcome.


