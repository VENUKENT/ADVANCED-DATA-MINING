---
title: "Assignment 2"
author: "Venu Dodda"
date: "04/08/2023"
output:
  word_document: default
  pdf_document: default
---
Question 1: What is the key idea behind bagging? Can bagging deal both with high variance (overfitting) and high bias (under fitting)?
```{r}
#The main goal of bagging, also known as bootstrap aggregating, is to increase a model's stability and accuracy by training several instances of the same model on various subsets of the training data, then combining their predictions.As part of the bagging procedure, many bootstrap samples—subsets of the training data selected at random with replacement—are created from the training data. After that, a different instance of the model is trained using each bootstrap sample. The final prediction is then created by combining all of the models' projections, often by taking the average or majority vote.
#Yes, bagging can help to deal with both high variance (overfitting) and high bias (underfitting) in a model.
#Bagging decreases the variance of the predictions by building numerous copies of the same model trained on various subsets of the training data. This prevents overfitting. A model can underperform when applied to fresh, untrained data due to overfitting, which happens when it is very sophisticated and fits the training data too closely. Bagging decreases the danger of overfitting by integrating the predictions of many models trained on various subsets of the data, resulting in more reliable and precise predictions.
#Bagging can also aid in lowering the bias of the predictions because each model is trained on a slightly different subset of the data. A model might perform badly on both the training and test sets of data due to underfitting, which happens when it is overly simplistic and fails to recognize the underlying patterns in the data. Bagging enables the models to explore more of the feature space and capture more of the underlying patterns, which can assist to decrease underfitting. It does this by training numerous models on various subsets of the data.
#Overall, bagging is an effective method that can assist to lower a model's high volatility and high bias, improving performance on fresh, untested data.

```
Question 2: Why bagging models are computationally more efficient when compared to boosting models with the same number of weak learners?
```{r}
#Due to the distinctions in the training procedures between boosting and bagging models, a bagging model often has a higher computational efficiency when compared to a boosting model with the same number of weak learners.
#Bagging is the process of training numerous instances of the same model separately using various subsets of the training data and replacement. The training process can be considerably sped up by being able to train every instance simultaneously. The instances may also be taught using various computational resources because they are independent, which can increase the process' efficiency even further.
#While weak learners are educated progressively in boosting, each learner is instructed to correct the mistakes made by the learners that came before them. Since that each weak learner depends on the one before them, this sequential training method may take longer than bagging. Although each weak learner is often more sophisticated than the individual examples used in bagging, boosting can also be more computationally costly. Because to the independent and concurrent training of the instances in bagging, as opposed to the sequential training process and possibly more sophisticated learners in boosting, bagging models are computationally more efficient when compared to boosting models with the same number of weak learners.
```
Question 3: James is thinking of creating an ensemble mode to predict whether a given stock will go up or down in the next week. He has trained several decision tree models but each model is not performing any better than a random model. The models are also very similar to each other. Do you think creating an ensemble model by combining these tree models can boost the performance?
```{r}
#With ensemble approaches like bagging and boosting, numerous weak learners are combined to produce a stronger, more precise model. The theory behind ensemble techniques is that by combining a number of weak models with different types of mistakes, the errors will balance out and the resultant model will be more accurate and less prone to overfitting. Nevertheless, integrating them is unlikely to result in a noticeable boost in performance if the individual models are highly similar to one another and are not outperforming a random model. Due to the ensemble model's increased complexity and the risk of overfitting, it can potentially result in worse performance.There might be a number of reasons why the individual decision tree models are not performing adequately. The models may be too complicated and overfit to the training set of data, for example. Overfitting is a problem with decision trees, especially when they are deep or the training set contains a lot of characteristics. If so, integrating the models using ensemble techniques could not be beneficial and might potentially make overfitting worse.Another possibility is that the models are missing key relationships or characteristics from the data. Decision trees can be biased and sometimes fail to recognize subtle or complicated links in the data. If this is the case, integrating the models might not solve the fundamental problems and might not considerably boost the performance. In general, it is crucial to make sure that the individual models are varied and commit distinct mistakes when considering ensemble approaches. This is so that ensemble techniques may combine the predictions of several independent and different models in order to lower the variance of the forecasts. Combining the models may not result in a noticeable performance gain if the separate models are too similar or consistently make the same mistakes.In conclusion, it seems doubtful that merging James' decision tree models will considerably improve performance if they are very close to one another and don't perform any better than a random model. To increase the performance of the model, he can like to experiment with various modeling approaches or look into additional characteristics and data sources. Instead, he can look for and fix the causes of the decision tree models' subpar performance before contemplating ensemble approaches.
```
Question 4:Consider the following Table that classifies some objects into two classes of edible (+) and non- edible (-), based on some characteristics such as the object color, size and shape. What would be the Information gain for splitting the dataset based on the “Size” attribute?
```{r}
#Entropy for our data set: I(all _ data) = -[(9/16)log 2(9/16)+(7/16)log2(7/16)] = 0.98361
#The entropy of small size = 0.811278 & The entropy of large size = 0.954434.
#Using this formula, we can calculate the Information Gain to be 0.105843.
#Information gain reveals the significance of a specific feature vector property. So, in this instance, the size attribute's information gain is. It's 0.10578144 crucial.
```
Question 5: Why is it important that the m parameter (number of attributes available at each split) to be optimally set in random forest models? Discuss the implications of setting this parameter too small or too large.
```{r}
#Each node will choose almost all characteristics if the "m" parameter is set to an extremely large value that is close to the total number of features ("p"), resulting in a lack of variety across different trees. As a result, the model will grow overly complicated and have a high variance, which might result in overfitting. On the other hand, if the "m" parameter is set too low, the amount of characteristics that can be represented by each node will be constrained, which may make it harder for the decision tree to capture significant correlations between the features. As a result, the model will be too simple and biased, which might cause underfitting. Consequently, in Random Forest models, it's crucial to adjust the "m" parameter to a suitable value. The ideal value for "m" depends on the particular issue at hand and, in actuality, should be seen as a tuning parameter. Generally speaking, a reasonable place to start is to set "m" to the logarithm base 2 of the number of features for regression issues and to the square root of the number of features for classification problems. To discover the ideal value that balances bias and variation, it is required to experiment with various values of "m." It's crucial to remember that Random Forest models are made to take use of the variety of individual decision trees, and that achieving this aim by setting "m" to an ideal number can assist.

```
```{r}
#Loading the Required Packages 
library(ISLR)
library(dplyr)
library(glmnet)
library(caret)
```
```{r}
#Using dplyr to select sales, price, advertising, population, age, income, and education.
Carseats_Filtered <- Carseats %>% select("Sales", "Price", 
"Advertising","Population","Age","Income","Education")

```
QB1. Build a decision tree regression model to predict Sales based on all other attributes 
("Price", "Advertising", "Population", "Age", "Income" and "Education").  Which attribute is used 
at the top of the tree (the root node) for splitting? Hint: you can either plot () and text()  
functions or use the summary() function to see the decision tree rules.  
```{r}
#Loading the Required Packages
library(rpart)
library(rpart.plot)
carsdata <- Carseats_Filtered
M1 = rpart(Sales~.,data=carsdata, method='anova')
#Summary of the Model 1
summary(M1)
```


```{r}
#Plotting the M1 
plot(M1)
text(M1)
```
#The attribute that is at the top of the tree is Price.

#Question #2: Consider the following input:Sales=9, Price=6.54, Population=124, Advertising=0, Age=76, Income= 110, Education=10. What will be the estimated Sales for this record using the decision tree model?

```{r}
M_2 = rpart(Sales~.,data=carsdata, method='anova', control = rpart.control(minsplit = 60 ))
summary(M_2)
```

```{r}
plot(M_2)
text(M_2)
```

```{r}
New_Model <- data.frame(Price=6.54,  Population=124, Advertising=0, Age=76, Income= 110, Education=10)
predict(M1, newdata=New_Model)
```

#The estimated sales for this record using a decision tree model is 9.5862.

#Question 3:  Use the caret function to train a random forest (method=’rf’) for the same dataset. Use the caret default settings. By default, caret will examine the “mtry” values of 2,4, and 6. Recall that mtry is the number of attributes available for splitting at each splitting node. Which mtry value gives the best performance?

```{r}
size = floor(0.70*nrow(Carseats_Filtered))
size
```

```{r}
set.seed(123)
train = sample(seq_len(nrow(Carseats_Filtered)), size = size)

TrainData = Carseats_Filtered[train,]
TestData = Carseats_Filtered[-train,]
rf_tree <- train(Sales~., data = Carseats_Filtered, method = "rf")
print(rf_tree)
```

#The mtry that gives the best performance is the 2nd mtry. 


#Question 4: Customize the search grid by checking the model’s performance for mtry values of 2, 3 and 5 using 3 repeats of 5-fold cross validation


```{r}
#Using the mtry value of 2
mtry = 2
Train_2 <- trainControl(method = "repeatedcv", number = 5, repeats = 3)
tunegrid1 <- expand.grid(.mtry=mtry)
tree_2 <- train(Sales~.,
               method = "rf",
               data = TrainData,
               trControl = Train_2,
               tuneGrid=tunegrid1
               )
print(tree_2)
```


```{r}
#Using mtry value of 3
mtry = 3
Train_2 <- trainControl(method = "repeatedcv", number = 5, repeats = 3)
tunegrid <- expand.grid(.mtry=mtry)
tree_2 <- train(Sales~.,
               method = "rf",
               data = TrainData,
               trControl = Train_2,
               tuneGrid=tunegrid
               )
print(tree_2)
```
```{r}

#Using mtry value of 5
mtry = 5
Train_2 <- trainControl(method = "repeatedcv", number = 5, repeats = 3)
tunegrid <- expand.grid(.mtry=mtry)
tree_2 <- train(Sales~.,
               method = "rf",
               data = TrainData,
               trControl = Train_2,
               tuneGrid=tunegrid
               )
print(tree_2)

```
