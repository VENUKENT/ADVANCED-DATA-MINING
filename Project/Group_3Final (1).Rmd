---
title: "ADM_Final_Group_3"
output: pdf_document
date: "2023-05-04"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


#Loading necessary poackages for current project:
```{r}
library(caret)
library(glmnet)
library(pls)
library(dplyr)
library(esquisse)
library(ggplot2)
library(randomForest)
```

#Loading train dataset:
```{r}
bank_tain_data<-read.csv("C:/Users/Pavan Chaitanya/Downloads/train_v3.csv")
```


```{r}
bank_tain_data$default <- factor(ifelse(bank_tain_data$loss > 0, 1, 0))

bank_tain_data$loss <- (bank_tain_data$loss / 100)
```


```{r}
row_missing <- rowMeans(is.na(bank_tain_data))

min_missing_values<-min(row_missing)
min_missing_values
max_missing_values<-max(row_missing)
max_missing_values
```

```{r}

ggplot(bank_tain_data, aes(x=factor(default))) +
  geom_bar(stat="count", width=0.4, fill="red") +
  labs(title="Non-Default v/s Default") +
  labs(x="", y="No. of Customers") +
  theme(plot.title = element_text(hjust = 0.4)) +
  geom_text(stat='count', aes(label=..count..), vjust=2)
```


#Removing the zero-variances variables and Preprocessing the dataset by removing highly correlated and imputing missing values using "corr" and "medianimpute":
```{r}
zero_var_indices <- nearZeroVar(bank_tain_data[ ,-c(763,764)])

data_cleaned <- bank_tain_data[, -zero_var_indices]

bank_preprocess <- preProcess(data_cleaned[ ,-c(739,740)], method = c("corr", "medianImpute"))

new_bank_tain_data <- predict(bank_preprocess, data_cleaned)
```




#1.CLASSIFICATION MODEL:


#a).Lasso Model: we now run the new_bank_tain_data with 248 attributes for variable selection:
```{r}
set.seed(843)

y <- as.vector(as.factor(new_bank_tain_data$default))

x <- data.matrix(new_bank_tain_data[,-c(247,248)])

lasso_model<- lasso_model<- cv.glmnet(x, y, alpha = 1,  preProcess = c("center", "scale"), family = "binomial", nfolds = 10, type.measure = "auc")

plot(lasso_model)

lasso_model$lambda.min
```

#Minimum Lambda value returned a total of 180 attributes out of 248 attributes:

```{r}
# Return the coefficients for the lasso regression at the minimum lambda value:
coef <- coef(lasso_model, s= "lambda.min")

#Convert the coefficient values into a dataframe:
lasso_lambda_coef<- data.frame(name = coef@Dimnames[[1]][coef@i + 1], coefficient = coef@x)

#Removing negatives values using "abs" function:
lasso_lambda_coef$coefficient <- abs(lasso_lambda_coef$coefficient)

#Re-arranging the data frame in decreasing order:
lasso_lambda_coef[order(lasso_lambda_coef$coefficient, decreasing = TRUE), ]

#Removing intercept columns returned from lasso model:
lasso_lambda_coef<- lasso_lambda_coef[-1, ]

#Converting the data frame to a vector:
lasso_lambda_coef<- as.vector(lasso_lambda_coef$name)

#Adding "default" column to the data frame:
lasso_lambda_coef<- c(lasso_lambda_coef,"default")

#Selecting attributes from original data set "new_bank_tain_data" using coefficients returned from lasso model i.e., "lasso_lambda_coef"
bank_lasso<-select(new_bank_tain_data, lasso_lambda_coef)


```





#b). Principle Component Analysis (PCA): 



```{r}
pca_model <- preProcess(bank_lasso[,-c(181)], method = c("center", "scale", "pca"), thresh = 0.80)

pca_model_1<- predict(pca_model, bank_lasso)

pca_model
```


#We are adding default column from previous model to the PCA model:
```{r}
pca_model_1$default <- bank_lasso$default
```


#Creating a train and validation sets from the values returned in PCA model:
```{r}
set.seed(843)

pca_index <- createDataPartition(pca_model_1$default, p = 0.80, list = FALSE)

pca_train <- pca_model_1[pca_index, ]
pca_validate <- pca_model_1[-pca_index, ] 

```


#Coverting the "default" column into factor in both train and validation sets:
```{r}
pca_train$default <- as.factor(pca_train$default)
pca_validate$default <- as.factor(pca_validate$default)
```


#Now run the values returned from PCA in random forest model:
```{r}
set.seed(843)

rfmodel <- randomForest(default ~ ., data = pca_train, mtry = 5)

print(rfmodel)
```




```{r}
pca_final <- data.frame(actual = pca_validate$default,predict(rfmodel, newdata = pca_validate, type = "prob"))

pca_final$predict <- ifelse(pca_final$X0 > 0.60, 0, 1)

pca <- confusionMatrix(as.factor(pca_final$predict), as.factor(pca_final$actual),positive='1')

pca
```



#Laoding Test data set for predicting the defaulting customers: 
```{r}
pca_test_set <- read.csv("C:/Users/Pavan Chaitanya/Downloads/test__no_lossv3.csv")
```

#We are imputing missing values same as we did for train data set using medianimpute method:
```{r}
test_pca_1 <- preProcess(pca_test_set, method = c("medianImpute"))

test_pca_process<- predict(test_pca_1, pca_test_set)

```


#Selecting attributes from test data set "test_pca_process" using coefficients returned from lasso model i.e., "lasso_lambda_coef":
```{r}
test_pca_lasso<-select(test_pca_process, lasso_lambda_coef[lasso_lambda_coef!="default"])
```


#We are processing test model also in PCA to match our train model:
```{r}
set.seed(843)
test_pca_model <- preProcess(test_pca_lasso, method = c("center", "scale", "pca"), thresh = 0.80)

test_pca_model_1<- predict(pca_model, test_pca_lasso)
```

#Predicting the test_pca_model_1 using the random forest model "rfmodel":
```{r}
set.seed(843)
predictions_pca <-data.frame(id=pca_test_set$id,predict(rfmodel, test_pca_model_1, type = "prob"))

threshold <- 0.60
predictions_pca$predicted_default <- ifelse(predictions_pca$X0 > threshold, 0, 1)
```

#Filtering the number defaulting customers that was predicting by our random forest model:
```{r}
filtered<-predictions_pca %>% filter(predicted_default == 1)
filtered
```

#Based on the results our random forest model predicted 33 customer will default in the test data set.


#We are binding our results from our classification model to our orginal test dataset:
```{r}
test_2<-pca_test_set
test_2$predictions <- predictions_pca$predicted_default
test_3<- test_2 %>% filter(predictions==1)

```


#2.REGRESSION MODEL:

#Laoding the train data set:
```{r}
new_train <- read.csv("C:/Users/Pavan Chaitanya/Downloads/train_v3.csv")
```


```{r}
new_train_1 <- new_train %>% filter(loss!=0) 
new_train_1$loss<- (new_train_1$loss / 100)
```

```{r}
zero_var_indices_1 <- nearZeroVar(new_train_1[ ,-c(763)])

trained_model_data <- new_train_1[, -zero_var_indices_1]

new_train_3 <- preProcess(trained_model_data[ ,-c(748)], method = c("medianImpute", "corr"))

new_train_4 <- predict(new_train_3, trained_model_data)
```


#Lasso model: we are using lasso model for variable selection for the dataset "new_train_4" consisting of 252 attributes:
```{r}
set.seed(843)
x_1 <- as.matrix(new_train_4[ ,-c(252)])
y_2 <- as.vector(new_train_4$loss)

model_lasso <- cv.glmnet(x_1, y_2, alpha = 1, family = "gaussian", nfolds = 10, type.measure = "mse")

plot(model_lasso)

model_lasso$lambda.min
```


#We convert coefficients returned in lasso model into dataframe:
```{r}
# Return the coefficients for the lasso regression at the minimum lambda value:
coef_test <- coef(model_lasso, s= "lambda.min")

#Convert the coefficient values into a data frame:
coef_test<- data.frame(name = coef_test@Dimnames[[1]][coef_test@i + 1], coefficient = coef_test@x)

#Removing negatives values using "abs" function:
coef_test$coefficient <- abs(coef_test$coefficient)

#Re-arranging the data frame in decreasing order:
coef_test[order(coef_test$coefficient, decreasing = TRUE), ]

#Removing intercept columns returned from lasso model:
coef_test<- coef_test[-1, ]

#Converting the data frame to a vector:
coef_test<- as.vector(coef_test$name)

#Adding "loss" column to the data frame:
coef_test<- c(coef_test,"loss")


#Selecting attributes from original data set "new_train_4" using coefficients returned from lasso model i.e., "coef_test"
final_model<-select(new_train_4, coef_test)
```


```{r}
set.seed(843)

bank_index_1 <- createDataPartition(final_model$loss, p = 0.80, list = FALSE)

bank_train_1 <- final_model[bank_index_1, ]
bank_validate_1 <- final_model[-bank_index_1, ]
```



```{r}
x_3 <- as.matrix(bank_train_1[ ,-c(121)])
y_3 <- as.vector(bank_train_1$loss)

ridge_model_data<- cv.glmnet(x_3, y_3, alpha = 0, family = "gaussian", nfolds = 10, type.measure = "mae")
```


```{r}
plot(ridge_model_data)

ridge_model_data$lambda.min

coef_final <- coef(ridge_model_data, s = "lambda.min")
```

## validating the Ridge model using "bank_validate_1" using "MAE" metrics:
```{r}
x_4 <- as.matrix(bank_validate_1[ ,-c(121)])
y_4 <- as.vector(bank_validate_1$loss)

predicted_loss <- predict(ridge_model_data, s = ridge_model_data$lambda.min, newx = x_4)

## Evaluating Performance.

MAE_lgd <- mean(abs((predicted_loss - y_4)))
comparison <- cbind(y_4,predicted_loss)

print(MAE_lgd)
```

#Selecting attributes from original data set "test_3" using coefficients returned from lasso model i.e., "coef_test"
```{r}
predict_9595<-select(test_3, coef_test[coef_test!="loss"])
```

#Imputing missing values in updated dataset "predict_9595":
```{r}
set.seed(843)
final_preprocess <- preProcess(predict_9595, method = c("medianImpute"))

final_preprocess_1 <- predict(final_preprocess, predict_9595)
```

#Predciting loss using ridge model by defaulting customers:
```{r}
default_loss<-as.data.frame(round(abs(predict(ridge_model_data, s = ridge_model_data$lambda.min, newx = as.matrix(final_preprocess_1)))*100))
```

#Storing loss given default values into a csv file:
```{r}
loss_given_default_data <- cbind.data.frame(filtered, default_loss)

s<-left_join(predictions_pca,loss_given_default_data,by='id')

s$loss <- ifelse(s$predicted_default.x==0,0,s$s1)

final_predicted_file<-data.frame(id=s$id,loss=s$loss)

write.csv(final_predicted_file, "final_predicted_file.csv")
```

